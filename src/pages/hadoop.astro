---
import Layout from './layout.astro';
import { Image } from 'astro:assets';
import { CopyIcon } from '@lucide/astro';
import { map_reduce_code } from '@/helper';
import { Button } from '@/lib/components/ui/_button';
import { LinkIcon } from '@lucide/astro';
const ids: string[] = ['#overview', '#paradigms', '#hdfs', '#map-reduce', '#yarn', '#hadoop-io'];
---

<script>
  import { map_reduce_code } from '@/helper';
  const copyBtn = document.getElementById('copy-code');
  copyBtn?.addEventListener('click', () => {
    navigator.clipboard.writeText(map_reduce_code);
  });
</script>

<Layout title="Hadoop">
    <section id="overview" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <div class="flex items-center justify-center flex-wrap">
            {ids.map((id) => (
                <Button variant="link">
                    <a href={id} class="text-blue-500 hover:text-blue-700 flex items-center justify-center flex-wrap">
                        {id.replace('#', '')}<LinkIcon />
                    </a>
                </Button>
            ))}
        </div>
        <div class="flex items-center justify-center flex-wrap">
            <h1>Apache Hadoop</h1>
            <div id="logo">
                <Image
                  src="https://hadoop.apache.org/elephant.png"
                  alt="Apache Hadoop Logo"
                  width={90}
                  height={90}
                  priority
                />
            </div>
        </div>
    </section>
    <section id="paradigms" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <!-- OSS -->
        <h3>Open Source Software: </h3>
        <ul class="list-item list-none">
            <li>The term Open-source is closely related to Open-source software (OSS). Open-source software is a type of computer software that is released under a license, but the source code is made available to all the users. The copyright holders of such software allow the users to use it and do some valuable modifications in its source code to add some new features, to improve the existing features, and to fix bugs if there are any. Because of this reason only Open-source software is mostly developed collaboratively.</li>
            <li>
                <p class="underline text-left">Examples</p>
                <ul class="text-left">
                    <li>
                        Operating systems - Android, Ubuntu, Linux
                    </li>
                    <li>
                        Internet browsers - Mozilla Firefox, Chromium
                    </li>
                    <li>
                        Integrated Development Environment (IDEs) - Vs code (Visual Studio Code), Android Studio, PyCharm, Xcode
                    </li>
                </ul>
            </li>
        </ul>
        <!-- Cloud and Big Data -->
        <h3>Hadoop with Cloud: </h3>
        <ul class="list-item list-none">
            <li>
                While Hadoop traditionally runs on-premises using commodity hardware, deploying it on cloud platforms provides greater flexibility and scalability.
            </li>
            <li>
                Integrating Hadoop with cloud platforms gives organizations the best of both worlds – Hadoop’s big data processing capabilities and the cloud’s scalability, cost savings, and ease of use. With managed services like AWS EMR, Azure HDInsight, and Google Dataproc, businesses can quickly deploy, process, and analyze massive datasets without worrying about infrastructure management. This combination is the foundation for modern, data-driven enterprises.
            </li>
            <li>
                <p class="underline text-left">Key Benefits:</p>
                <ul class="text-left">
                    <li>
                        Elastic Scalability – Easily scale Hadoop clusters up or down depending on workload.
                    </li>
                    <li>
                        Internet browsers - Mozilla Firefox, Chromium
                    </li>
                    <li>
                        Integrated Development Environment (IDEs) - Vs code (Visual Studio Code), Android Studio, PyCharm, Xcode
                    </li>
                </ul>
            </li>
            <li>
                <p class="underline text-left">Examples:</p>
                <ul class="text-left">
                    <li>Amazon EMR (Elastic MapReduce) provides a managed Hadoop service.</li>
                    <li>Azure HDInsight offers a cloud-based Hadoop service.</li>
                    <li>GCP Dataproc is Google’s managed Hadoop and Spark service.</li>
                    <li>IBM Cloud Provides Analytics Engine for Hadoop and Spark.</li>
                </ul>
            </li>
        </ul>
        <!-- Cloud computing -->
        <h3>Cloud Computing:</h3>
        <ul class="list-item list-none">
            <li>
                Cloud computing is a technology that allows you to store and access data, applications, and computing resources over the internet instead of relying on your computer's hard drive or local servers. It provides on-demand access to services like storage, databases, servers, and software, which are hosted on remote servers managed by cloud providers
            </li>
            <li>
                Cloud computing has transformed how businesses and individuals manage data, offering flexibility, reliability, and cost savings. Whether you're storing personal files or running a large enterprise, the cloud makes it simpler and more efficient.
            </li>
            <li>
                <p class="underline text-left">Key Benefits:</p>
                <ul class="text-left">
                    <li>
                        Infrastructure as a Service (IaaS): Provides virtual servers, storage, and networks. You manage the software, while the provider handles the hardware.
                    </li>
                    <li>
                        Platform as a Service (PaaS): Offers tools and platforms for developers to build and deploy applications without worrying about infrastructure.
                    </li>
                    <li>
                        Software as a Service (SaaS): Lets you use software directly online, like Gmail or Dropbox, without installation or maintenance.
                    </li>
                </ul>
            </li>
            <li>
                <p class="underline text-left">Examples:</p>
                <ul class="text-left">
                    <li>Amazon EMR (Elastic MapReduce) provides a managed Hadoop service.</li>
                    <li>Azure HDInsight offers a cloud-based Hadoop service.</li>
                    <li>GCP Dataproc is Google’s managed Hadoop and Spark service.</li>
                    <li>IBM Cloud Provides Analytics Engine for Hadoop and Spark.</li>
                </ul>
            </li>
        </ul>
    </section>
    <section id="hdfs" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <h1 class="mb-4 underline">Hadoop Distributed File System (HDFS)</h1>
        <!-- Overview -->
        <h3>Overview: </h3>
        <ul class="list-item list-none">
            <li>                The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project.</li>
            <li>
                <p class="underline text-left">Examples</p>
                <ul class="text-left">
                    <li>
                        Operating systems - Android, Ubuntu, Linux
                    </li>
                    <li>
                        Internet browsers - Mozilla Firefox, Chromium
                    </li>
                    <li>
                        Integrated Development Environment (IDEs) - Vs code (Visual Studio Code), Android Studio, PyCharm, Xcode
                    </li>
                </ul>
            </li>
        </ul>
        <!-- HDFS Architectire image -->
        <div id="picture" class="w-fit">
            <Image
                src="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsarchitecture.png",
                alt="HDFS Architecture"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- NameNode -->
        <h3 class="underline">NameNode: </h3>
        <ul class="text-left">
            <li>
                An HDFS cluster consists of a single <span class="underline">NameNode</span>, a master server that manages the file system namespace and regulates access to files by clients.  The NameNode executes file system namespace operations like opening, closing, and renaming files and directories.<br /> It also determines the mapping of blocks to DataNodes. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system.<br /> The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.
            </li>
            <li>
                The primary purpose of Namenode is to manage all the MetaData. Metadata is the list of files stored in HDFS(Hadoop Distributed File System)
            </li>
            <li>
                All information regarding the logs of the transactions happening in a Hadoop cluster (when or who read/wrote the data) will be stored in MetaData. MetaData is stored in the memory.
            </li>
            <li>
                <p>Starting of NameNode</p>
                <pre>hadoop-daemon.sh start namenode</pre>
            </li>
            <li>
                <p>Stopping of NameNode</p>
                <pre>hadoop-daemon.sh stop namenode</pre>
            </li>
        </ul>
        <!-- DataNode -->
        <h3>DataNode: </h3>
        <ul class="text-left">
            <li>
                In addition, there are a number of <span class="underline">DataNodes</span>, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. <br />Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. <br />The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
            </li>
            <li>
                The NameNode always instructs DataNode for storing the Data. DataNode is a program that runs on the slave system that serves the read/write request from the client.
            </li>
            <li>
                <p>Starting of DataNode</p>
                <pre>hadoop-daemon.sh start datanode</pre>
            </li>
            <li>
                <p>Stopping of DataNode</p>
                <pre>hadoop-daemon.sh stop datanode</pre>
            </li>
        </ul>
        <!-- Secondary NameNode -->
        <h3>Secondary NameNode: </h3>
        <ul class="text-left">
            <li>
                <span class="underline">Secondary NameNode</span> is used for taking the hourly backup of the data. In case the Hadoop cluster fails, or crashes, the secondary Namenode will take the hourly backup or checkpoints of that data and store this data into a file name fsimage. <br /> This file then gets transferred to a new system. A new MetaData is assigned to that new system and a new Master is created with this MetaData, and the cluster is made to run again correctly.
            </li>
            <li>
                This file then gets transferred to a new system. A new MetaData is assigned to that new system and a new Master is created with this MetaData, and the cluster is made to run again correctly.
            </li>
        </ul>
        <!-- Secondary NameNode Image -->
        <div id="picture" class="w-fit">
            <Image
                src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20200820173200/secondary-namenode.png",
                alt="HDFS Architecture"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- Block Size -->
        <h3>Block Size: </h3>
        <ul class="text-left">
            <li>
                HDFS is used in Hadoop to store the data means all of our data is stored in HDFS. Hadoop is also known for its efficient and reliable storage technique.
            </li>
            <li>
                By default in Hadoop1, these blocks are 64MB in size, and in Hadoop2 these blocks are 128MB in size which means all the blocks that are obtained after dividing a file should be 64MB or 128MB
            </li>
            <li>
                By default in Hadoop1, these blocks are 64MB in size, and in Hadoop2 these blocks are 128MB
            </li>
            <li>
                Suppose you have uploaded a file of 400MB to your HDFS then what happens is, this file got divided into blocks of 128MB + 128MB + 128MB + 16MB = 400MB size. Means 4 blocks are created each of 128MB
            </li>
        </ul>
        <!-- Block Size Image -->
        <div id="picture" class="w-fit">
            <Image
                src="https://media.geeksforgeeks.org/wp-content/uploads/20200618125555/3164-1-768x359.png",
                alt="Data Replications"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- Features -->
        <h3>Features:</h3>
        <ul class="text-left">
            <li>
                    <span class="underline">Hardware Tolerant: </span> <br />
                    Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
            </li>
            <li>

                    <span class="underline">Large Data Sets: </span> <br />
                    Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
            </li>
            <li>

                    <span class="underline">Simple Coherency Model: </span> <br />
                    HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed except for appends and truncates. Appending the content to the end of the files is supported but cannot be updated at arbitrary point. This assumption simplifies data coherency issues and enables high throughput data access

            </li>
            <li>

                    <span class="underline">Portability: </span> <br />
                    HDFS has been designed to be easily portable from one platform to another

            </li>
        </ul>
        <!-- Data Replication -->
        <h3>Data Replication: </h3>
        <ul class="text-left">
            <li>
                HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file.
            </li>
            <li>
                All blocks in a file except the last block are the same size, while users can start a new block without filling out the last block to the configured block size after the support for variable length block was added to append and hsync.
            </li>
            <li>
                The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
            </li>
        </ul>
        <!-- Data Replication Image -->
        <div id="picture" class="w-fit">
            <Image
                src="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png",
                alt="Data Replications"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- Inter Process Communication -->
        <h3>Inter ProcessCommunication:</h3>
        <ul class="text-left">
            <li>
                All HDFS communication protocols are layered on top of the TCP/IP protocol
            </li>
            <li>
                A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode
            </li>
            <li>
                The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol.
            </li>
        </ul>
    </section>
    <section id="map-reduce" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <h1 class="underline">MapReduce</h1>
        <!-- Overview -->
        <h3>Overview: </h3>
        <ul class="text-left">
            <li>
                MapReduce is a programming model and an associated implementation for processing and generating large data sets with a parallel, distributed algorithm on a cluster. It is designed to handle big data by dividing the task into smaller sub-tasks, which are processed in parallel across multiple nodes in a cluster.
            </li>
            <li>
                Hadoop MapReduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner.
            </li>
            <li>
                A MapReduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks.
            </li>
            <li>

            </li>
        </ul>
        <!-- MapReduce Architecture -->
        <div id="picture" class="w-fit">
            <Image
                src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20200908123810/MapReduce-Architecture.jpg",
                alt="MapReduce Architecture"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- Code -->
        <h3>Code: </h3>
        <div class="prose lg:prose-xl max-w-screen">
            <Button variant="secondary" size="icon-sm" id="copy-code" className="relative top-0 right-0">
                <CopyIcon />
            </Button>
            <pre class="text-left relative top-0 right-0 max-w-fit text-wrap">
                {map_reduce_code}
            </pre>
        </div>
        <!-- Map Phase -->
        <h3>Map Phase</h3>
        <ul class="text-left">
            <li>The input dataset is divided into splits, each processed by a Map Task on the node storing the data (ensuring data locality).</li>
            <li>A RecordReader converts raw input into (key, value) pairs, which the Mapper transforms into intermediate results (e.g., "Hello Hadoop" → (Hello, 1), (Hadoop, 1)).</li>
            <li>The Map Phase generates but does not aggregate data.</li>
        </ul>
        <!-- Shuffle & Sort Phase -->
        <h3>Shuffle & Sort Phase</h3>
        <ul class="text-left">
            <li>After mapping, the intermediate outputs are reorganized so they can be processed efficiently by reducers. Shuffling groups identical keys, e.g., (Hadoop, 1), (Hadoop, 1), (Hadoop, 1) becomes (Hadoop, [1,1,1]).</li>
            <li>Sorting then arranges the keys in order. This stage ensures balanced distribution of work and is essential for linking the Map and Reduce phases.</li>
        </ul>
        <!-- Reduce Phase -->
        <h3>Reduce Phase</h3>
        <ul class="text-left">
            <li>
                Finally, reducers take the grouped keys and their associated values from the Shuffle & Sort stage.
            </li>
            <li>The reduce() function aggregates or summarizes them to produce the final output. For example, (Hadoop, [1,1,1]) becomes (Hadoop, 3). The consolidated results are written back into HDFS at the client-specified output location.</li>
        </ul>
        <!-- MapReduce DataFlow -->
        <div id="picture" class="w-fit">
            <Image
                src="https://media.geeksforgeeks.org/wp-content/cdn-uploads/20200729213509/Hadoop-MapReduce-Data-Flow.png",
                alt="MapReduce Data Flow"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
        <!-- Job Tracker -->
        <h3>Job Tracker</h3>
        <ul class="text-left">
            <li>JobTracker is an essential Daemon for MapReduce execution, it process runs on a separate node and not usually on a DataNode.</li>
            <li>JobTracker receives the requests for MapReduce execution from the client, talks to the NameNode to determine the location of the data.</li>
            <li>JobTracker monitors the individual TaskTrackers and the submits back the overall status of the job back to the client.</li>
        </ul>
        <!-- Task Tracker -->
        <h3>Task Tracker</h3>
        <ul class="text-left">
            <li>
                TaskTracker runs on DataNode. Mostly on all DataNodes, is replaced by Node Manager in MRv2
            </li>
            <li>Mapper and Reducer tasks are executed on DataNodes administered by TaskTrackers.</li>
        </ul>
    </section>
    <section id="yarn" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <h1 class="underline">Yet Another Resource Manager (YARN)</h1>
        <!-- Overview -->
        <h3>Overview: </h3>
        <ul class="text-left">
            <li>
                YARN is resource manager layer in Hadoop ecosystem.
            </li>
            <li>
                The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM)
            </li>
        </ul>
        <!-- YARN Flow -->
        <div id="picture" class="w-fit">
            <Image
                src="https://media.geeksforgeeks.org/wp-content/uploads/HadoopYarn.jpg",
                alt="YARN Flow"
                loading="lazy"
                layout="full-width"
                inferSize
                class="brightness-75"
            />
        </div>
    </section>
    <section id="hadoop-io" class="prose lg:prose-xl center min-w-screen p-5  text-white">
        <h1 class="underline">Hadoop IO</h1>
        <!-- Overview -->
        <h3>Overview: </h3>
        <ul class="text-left">
            <li>
                Hadoop Comes with a set of primitives for data  I/O.
            </li>
            <li>
                Fundamental idea of Hadoop is big data processing, hence it is designed for massive data processing, provides a set of APIs for reading and writing data.
            </li>
        </ul>
        <!-- Data Integrity -->
        <h3>Data Integrity</h3>
        <ul class="text-left">
            <li>In Hadoop, data integrity is crucial for ensuring the accuracy and reliability of large datasets.</li>
            <li>
                Given the distributed nature of Hadoop, data integrity is maintained through a combination of checksums, replication, and fault tolerance mechanisms.
            </li>
            <li>CRC 32 is typically used for error detection in Hadoop. Computed when data is written and verified during read operations.</li>
        </ul>
        <!-- Compressions -->
        <h3>Data Compression</h3>
        <ul class="text-left">
            <li>GZIP: Good compression ratio, slow, unsplitable, widely used for general purposes.</li>
            <li>BZIP2: Parallel, Splitable, higher compression ratio than GZIP but slower</li>
            <li>LZO, LZ4 and Snappy: Lower compression ratio than GZIP and BZIP2, optimized for speed and low memory usage. LZO and Snappy are splittable</li>
        </ul>
    </section>
</Layout>
